# Config-Based LLM Fine-Tuning & Test Platform

Bu proje, NVIDIA GPU'lar Ã¼zerinde LLM (Large Language Model) fine-tuning (ince ayar) iÅŸlemleri yapmak, modelleri test etmek ve karÅŸÄ±laÅŸtÄ±rmak iÃ§in geliÅŸtirilmiÅŸ konfigÃ¼rasyon tabanlÄ± bir platformdur.

## ğŸš€ Ã–zellikler

*   **Tamamen KonfigÃ¼re Edilebilir:** TÃ¼m eÄŸitim ve model ayarlarÄ± tek bir `config.yaml` dosyasÄ±ndan yÃ¶netilir.
*   **DÃ¼ÅŸÃ¼k VRAM Dostu:** 4-bit ve 8-bit quantization (QLoRA) desteÄŸi ile 12-16GB VRAM'de Ã§alÄ±ÅŸabilir.
*   **Kesintiye DayanÄ±klÄ± (Resume):** EÄŸitim yarÄ±da kalÄ±rsa, otomatik olarak son checkpoint'ten devam eder.
*   **ModÃ¼ler Mimari:** EÄŸitim, Test ve API katmanlarÄ± birbirinden ayrÄ±lmÄ±ÅŸtÄ±r.
*   **KarÅŸÄ±laÅŸtÄ±rma API'si:** Base model ile Fine-tuned modeli yan yana karÅŸÄ±laÅŸtÄ±ran REST API.

## ğŸ› ï¸ Kurulum

### Gereksinimler
*   Linux Ä°ÅŸletim Sistemi
*   NVIDIA GPU (Min. 12GB VRAM Ã¶nerilir)
*   Python 3.10+
*   CUDA Toolkit

### AdÄ±mlar

1.  **Projeyi KlonlayÄ±n:** (EÄŸer git kullanÄ±yorsanÄ±z)
    ```bash
    git clone <repo-url>
    cd AITraining2026
    ```

2.  **Sanal Ortam OluÅŸturun (Opsiyonel ama Ã–nerilir):**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin:**
    ```bash
    pip install -r requirements.txt
    ```

## âš™ï¸ KonfigÃ¼rasyon

Projenin kalbi `config.yaml` dosyasÄ±dÄ±r. Ã–rnek bir konfigÃ¼rasyon aÅŸaÄŸÄ±daki gibidir:

```yaml
model:
  name_or_path: "mistralai/Mistral-7B-v0.1" # HuggingFace model ID veya yerel yol
  quantization_bit: 4                       # 4 veya 8 (VRAM tasarrufu iÃ§in 4 Ã¶nerilir)
  use_gradient_checkpointing: true          # VRAM tasarrufu saÄŸlar

peft:
  r: 16                                     # LoRA rank
  lora_alpha: 32
  target_modules: ["q_proj", "v_proj"]      # Hangi modÃ¼llere uygulanacaÄŸÄ±

training:
  batch_size: 1
  gradient_accumulation_steps: 4            # DÃ¼ÅŸÃ¼k batch size'Ä± telafi etmek iÃ§in artÄ±rÄ±n
  num_train_epochs: 1
  learning_rate: 2.0e-4
  output_dir: "experiments/my_finetune"
  resume_from_checkpoint: true              # Otomatik devam etme Ã¶zelliÄŸi

data:
  dataset_path: "dataset.json"              # EÄŸitim verisi yolu
  max_seq_length: 512
```

## ğŸ‹ï¸â€â™‚ï¸ EÄŸitim (Fine-Tuning)

EÄŸitimi baÅŸlatmak iÃ§in aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python main.py --config config.yaml
```

*   **Ä°pucu:** EÄŸer `config.yaml` dosyasÄ±nda `resume_from_checkpoint: true` ise ve `output_dir` iÃ§inde daha Ã¶nce alÄ±nmÄ±ÅŸ bir kayÄ±t varsa, eÄŸitim kaldÄ±ÄŸÄ± yerden devam eder.
*   EÄŸitim sÄ±rasÄ±nda loglar ekrana ve `training.log` dosyasÄ±na basÄ±lÄ±r.

## ğŸ§ª Test ve KarÅŸÄ±laÅŸtÄ±rma API'si

EÄŸittiÄŸiniz modeli base model ile karÅŸÄ±laÅŸtÄ±rmak iÃ§in API servisini baÅŸlatÄ±n:

1.  **API Sunucusunu BaÅŸlatÄ±n:**
    ```bash
    uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
    ```
    *Not: API varsayÄ±lan olarak `config.yaml` dosyasÄ±nÄ± okur. FarklÄ± bir config iÃ§in `CONFIG_PATH` environment deÄŸiÅŸkenini kullanabilirsiniz.*

2.  **KarÅŸÄ±laÅŸtÄ±rma Ä°steÄŸi GÃ¶nderin:**
    Yeni bir terminal aÃ§Ä±p `curl` veya Postman ile test edebilirsiniz:

    ```bash
    curl -X POST "http://localhost:8000/compare" \
         -H "Content-Type: application/json" \
         -d '{"question": "Python nedir?"}'
    ```

    **Ã–rnek YanÄ±t:**
    ```json
    {
      "question": "Python nedir?",
      "base_model": {
        "answer": "...",
        "tokens_used": 150,
        "time_ms": 1200
      },
      "finetuned_model": {
        "answer": "...",
        "tokens_used": 145,
        "time_ms": 900
      }
    }
    ```

## ğŸ“‚ KlasÃ¶r YapÄ±sÄ±

*   `src/`: Kaynak kodlar (Trainer, Config, Utils, vb.)
*   `main.py`: EÄŸitim baÅŸlatma dosyasÄ±.
*   `config.yaml`: Ayar dosyasÄ±.
*   `dataset.json`: EÄŸitim veriseti.
*   `requirements.txt`: Python kÃ¼tÃ¼phaneleri.
